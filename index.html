<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- bulma css template -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- ionicons -->
  <script type="module" src="https://unpkg.com/ionicons@7.1.0/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@7.1.0/dist/ionicons/ionicons.js"></script>
  <!-- model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
  <title>
    DreamGaussian
  </title>
  <link rel="icon" href="icon.ico">
</head>
<body>
  <section class="section">

  <div class="container has-text-centered">
    <!-- paper title -->
    <p class="title is-3"> LoSF-UDF: Learning Unsigned Distance Fields from Local Shape Functions for 3D Surface Reconstruction </p>
    <!-- publication -->
    <p class="subtitle is-4"> -- </p>
    <!-- authors -->
    <p class="title is-5 mt-2"> 
      <a href="https://jbhu67.github.io/" target="_blank">Jiangbei Hu</a><sup>1,2</sup>, 
      Yanggeng Li<sup>1</sup>, 
      <a href="" target="_blank">Fei Hou</a><sup>3</sup>, 
      <a href="" target="_blank">Junhui Hou</a><sup>4</sup>,
      <a href="" target="_blank">Zhebin Zhang</a><sup>1</sup>
      <a href="" target="_blank">Shengfa Wang</a><sup>2</sup>
      <a href="" target="_blank">Na Lei</a><sup>2</sup>
      <a href="" target="_blank">Ying He</a><sup>1</sup>
    </p>
    <!-- affiliations -->
    <p class="subtitle is-5"> 
      <sup>1</sup> Nanyang Technological University &nbsp;
      <sup>2</sup> Dalian University of Technology &nbsp;
      <sup>3</sup> Chinese Academy of Sciences &nbsp;
      <sup>4</sup> City University of Hong Kong &nbsp;
      <sup>5</sup> InnoPeak Technology &nbsp;
    </p>

    <!-- other links -->
    <div class="is-flex is-justify-content-center">
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://arxiv.org/abs/2407.01330" role="button" target="_blank"> <span class="icon"> <ion-icon name="document-outline"></ion-icon> </span> <span> Arxiv </span>  </a> 
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://github.com/jbHu67/Local2UDF" role="button" target="_blank"> <span class="icon"> <ion-icon name="logo-github"></ion-icon> </span> <span> Code </span> </a> 
      </span>
    </div>
   
  </div>

  <!-- main container -->
  <div class="container is-max-desktop has-text-centered">

    <!-- abstract -->
    <p class="title is-3 mt-5 has-text-centered"> Abstract </p>
    <p class="content is-size-6 has-text-left">
      Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large datasets of 3D shapes, which is costly and often necessitates hyperparameter adjustments for new datasets. This paper presents a novel neural framework, <b>LoSF-UDF</b>, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns within localized areas, prompting us to create a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. This method enables efficient and robust surface reconstruction from point clouds without the need for shape-specific training. Additionally, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We present comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy.
    </p>
    
    <!-- results (videos) -->
    <p class="title is-3 mt-5 has-text-centered"> Convergence Speed </p>

    <p class="content has-text-centered is-size-5">
      Our method converges in 2 minutes for image-to-3D while keeping good generation quality.
    </p>
    <video muted autoplay controls loop> <source src="videos/accelerate.mp4" type="video/mp4"> </video>


    <p class="title is-3 mt-5 has-text-centered"> Image-to-3D </p>
    <video muted autoplay loop> <source src="videos/image1.mp4" type="video/mp4"> </video>
    <video muted autoplay loop> <source src="videos/image2.mp4" type="video/mp4"> </video>

    <p class="content has-text-centered is-size-5">
      We can support images with a non-zero elevation angle too!
    </p>

    <video muted autoplay loop> <source src="videos/elevation.mp4" type="video/mp4"> </video>
    
    <p class="title is-3 mt-5 has-text-centered"> Text-to-3D </p>
    <video muted autoplay loop> <source src="videos/text.mp4" type="video/mp4"> </video>

    <p class="title is-3 mt-5 has-text-centered"> Text-to-image-to-3D </p>
    <video muted autoplay loop> <source src="videos/text-to-image-to-3d.mp4" type="video/mp4"> </video>

    <p class="title is-3 mt-5 has-text-centered"> Optimization Progress </p>
    <table class="table">
      <tbody>
        <tr> 
          <th> <video muted controls autoplay loop> <source src="videos/gui.mp4" type="video/mp4"> </video> </th>
          <th> <video muted controls autoplay loop> <source src="videos/gui2.mp4" type="video/mp4"> </video> </th>
        </tr>
        <tr>
          <th class="has-text-centered"> Stage 1 (Generative Gaussian Splatting) </th>
          <th class="has-text-centered"> Stage 2 (Mesh Texture Refinement) </th>
        </tr>
    </table>
    <p class="content has-text-centered is-size-5">
      Videos are played at the original speed and recorded on an NVIDIA 3070 (8GB).
    </p>

    <!-- 3d model viewer -->
    <p class="title is-3 mt-5 has-text-centered"> Exported Meshes </p>
    <div class="level">
      <div class="level-item">
        <model-viewer src="meshes/anya.glb" poster="meshes/anya.jpg" style="width: 100%; height: 300px;" auto-rotate shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
      <div class="level-item">
        <model-viewer src="meshes/luigi.glb" poster="meshes/luigi.jpg" style="width: 100%; height: 300px;" auto-rotate shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
      <div class="level-item">
        <model-viewer src="meshes/zelda.glb" poster="meshes/zelda.jpg" style="width: 100%; height: 300px;" auto-rotate shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
      <div class="level-item">
        <model-viewer src="meshes/toy.glb" poster="meshes/toy.jpg" style="width: 100%; height: 300px;" auto-rotate shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
    </div>
    
    <p class="title is-3 mt-5 has-text-centered"> Mesh Animations </p>
    <p class="content has-text-centered is-size-5">
      The following results are animated by <a href="https://www.mixamo.com/">Mixamo</a>.
    </p>
    <div class="level">
      <div class="level-item">
        <model-viewer src="meshes/rabbit_animate.glb" poster="meshes/toy2.jpg" style="width: 100%; height: 300px;" autoplay shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
      <div class="level-item">
        <model-viewer src="meshes/girl_animate.glb" poster="meshes/girl_animate.png" style="width: 100%; height: 300px;" autoplay shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
      <div class="level-item">
        <model-viewer src="meshes/boy_animate.glb" poster="meshes/boy_animate.png" style="width: 100%; height: 300px;" autoplay shadow-intensity="1" camera-controls touch-action="pan-y"></model-viewer>
      </div>
    </div>

    <!-- citation -->
    <div class="card mt-4">
      <header class="card-header">
        <p class="card-header-title"> Citation </p>
      </header>
      <div class="card-content is-size-5 has-text-left">
<pre><code>@article{tang2023dreamgaussian,
  title={DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation},
  author={Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
  journal={arXiv preprint arXiv:2309.16653},
  year={2023}
}</code></pre>
      </div>
    </div>
  </div>


  </section>
</body>
</html>
